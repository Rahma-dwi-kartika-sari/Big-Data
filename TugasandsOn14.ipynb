{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cc1cb7-411e-4e7d-a3dd-2b747b35eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/24 12:51:55 WARN Utils: Your hostname, rahma, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/24 12:51:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/24 12:52:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/24 12:52:50 WARN Instrumentation: [42ccae65] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/24 12:52:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/24 12:53:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03827e4-94ba-429a-ad84-9ae5532f923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057936866103,4.0873522690497905]\n",
      "Intercept: 11.568912734332656\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT \n",
    "from pyspark.sql.functions import udf \n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
    "columns = ['ID', 'Features_Array', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a UDF to convert ArrayType to VectorUDT\n",
    "array_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "# Apply the UDF to create the 'Features' column directly\n",
    "df_transformed = df.withColumn(\"Features\", array_to_vector_udf(\"Features_Array\"))\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bf59cb-8cc6-4ef3-9854-418d7047b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT \n",
    "from pyspark.sql.functions import udf \n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [1.0, 1.0]), (2, [5.0, 5.0]), (3, [10.0, 10.0]), (4, [15.0, 15.0])]\n",
    "columns = ['ID', 'Features_Array'] \n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a UDF to convert ArrayType to VectorUDT\n",
    "array_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "# Apply the UDF to create the 'Features' column directly\n",
    "df_transformed = df.withColumn(\"Features\", array_to_vector_udf(\"Features_Array\"))\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df_transformed)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33ab3b5b-2b0a-41f9-887a-3743e59d2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6b5f8cf-e099-463c-83bd-6328b7aa39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8569ae2-821a-4ef4-9186-c6dea5ef3647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete.\n",
      "+--------------------+--------+------+----+---+-------------------+-----+-----+------+--------------------+-----+--------+\n",
      "|         PassengerId|Survived|Pclass|Name|Sex|                Age|SibSp|Parch|Ticket|                Fare|Cabin|Embarked|\n",
      "+--------------------+--------+------+----+---+-------------------+-----+-----+------+--------------------+-----+--------+\n",
      "|                 0.0|     0.0|   1.0| 108|  1| 0.2711736617240512|0.125|  0.0|   523|0.014151057562208049|  147|       2|\n",
      "|0.001123595505617...|     1.0|   0.0| 190|  0| 0.4722292033174164|0.125|  0.0|   596| 0.13913573538264068|   81|       0|\n",
      "|0.002247191011235955|     1.0|   1.0| 353|  0|0.32143754712239253|  0.0|  0.0|   669|0.015468569817999833|  147|       2|\n",
      "|0.003370786516853...|     1.0|   0.0| 272|  0| 0.4345312892686604|0.125|  0.0|    49| 0.10364429745562033|   55|       2|\n",
      "|0.004494382022471...|     0.0|   1.0|  15|  1| 0.4345312892686604|  0.0|  0.0|   472|0.015712553569072387|  147|       2|\n",
      "+--------------------+--------+------+----+---+-------------------+-----+-----+------+--------------------+-----+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"TitanicMLlibHomework\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\n",
    "    \"titanic_cleaned.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "data = data.withColumn(\"Survived\", data[\"Survived\"].cast(DoubleType()))\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'\n",
    "]\n",
    "LABEL_COL = 'Survived'\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLS,\n",
    "    outputCol=\"rawFeatures\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"Data Preparation Complete.\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "744be404-f2d6-4e02-a38f-a95712791316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koefisien (Vector):\n",
      "[-0.6997331007788276,-1.259628672572061,-0.4443307818378029,-0.47817359850907015,-0.0731722378047105,0.17298027075355055,-0.22634632112264633]\n",
      "\n",
      "Intercept (Scalar):\n",
      "3.708890815818252\n"
     ]
    }
   ],
   "source": [
    "lr_model_baseline = model_baseline.stages[-1]\n",
    "\n",
    "print(\"Koefisien (Vector):\")\n",
    "print(lr_model_baseline.coefficients) \n",
    "\n",
    "print(\"\\nIntercept (Scalar):\")\n",
    "print(lr_model_baseline.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95d9d31f-fd01-45cf-923d-d3ab6d1c4da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi Model Baseline (tanpa Tuning): 0.7931\n",
      "AUC Model Baseline (tanpa Tuning): 0.8851\n"
     ]
    }
   ],
   "source": [
    "pipeline_baseline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "model_baseline = pipeline_baseline.fit(train_data)\n",
    "\n",
    "predictions_baseline = model_baseline.transform(test_data)\n",
    "\n",
    "mc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=LABEL_COL,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy_baseline = mc_evaluator.evaluate(predictions_baseline)\n",
    "\n",
    "print(f\"Akurasi Model Baseline (tanpa Tuning): {accuracy_baseline:.4f}\")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=LABEL_COL,\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc_baseline = auc_evaluator.evaluate(predictions_baseline)\n",
    "print(f\"AUC Model Baseline (tanpa Tuning): {auc_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57e5a6-f547-4c36-88b9-d838f9ece553",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "    .addGrid(lr.maxIter, [10, 50, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline_baseline, \n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=auc_evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "predictions_tuned = cvModel.transform(test_data)\n",
    "accuracy_tuned = mc_evaluator.evaluate(predictions_tuned)\n",
    "auc_tuned = auc_evaluator.evaluate(predictions_tuned)\n",
    "\n",
    "best_lr_model = best_model.stages[-1]\n",
    "\n",
    "print(\"Performa Model Terbaik (Setelah Tuning) \")\n",
    "print(f\"Akurasi Model Terbaik (Tuned): {accuracy_tuned:.4f}\")\n",
    "print(f\"Area Under ROC (AUC) Model Terbaik (Tuned): {auc_tuned:.4f}\")\n",
    "\n",
    "print(\"\\nHyperparameters Terbaik\")\n",
    "print(f\"Best regParam: {best_lr_model.getRegParam()}\")\n",
    "print(f\"Best maxIter: {best_lr_model.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e248f-4684-4f50-b307-94323f430bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
